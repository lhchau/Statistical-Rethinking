---
title: "Untitled"
author: "Hoang-Chau Luong"
date: '2022-09-03'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Medium**

## **1**

Ice_cream_sales ~ Temperature + Shark_attacks

```{r}
library(tidyverse)
library(rethinking)
library(brms)
library(tidybayes)

theme_set(theme_light())

set.seed(2022)
n <- 100
temp <- rnorm(n)
shark <- rnorm(n, temp)
ice_cream <- rnorm(n, temp)

spur_exp <- tibble(ice_cream, temp, shark) %>% 
  mutate(across(everything(), standardize))

mod_t <- brm(ice_cream ~ 1 + temp, data = spur_exp, family = gaussian,
             prior = c(prior(normal(0, 0.2), class = Intercept),
                       prior(normal(0, 0.5), class = b),
                       prior(exponential(1), class = sigma)),
             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)

mod_s <- brm(ice_cream ~ 1 + shark, data = spur_exp, family = gaussian,
             prior = c(prior(normal(0, 0.2), class = Intercept),
                       prior(normal(0, 0.5), class = b),
                       prior(exponential(1), class = sigma)),
             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)

mod_all <- brm(ice_cream ~ 1 + temp + shark, data = spur_exp, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)
```

Biểu đồ dưới đây cho thấy các phân phối sau của các hệ số β cho các temperature và shark attacks. Đúng như dự đoán, cả hai đều có mối quan hệ tích cực với doanh số bán kem trong các mô hình bivariate. Tuy nhiên, khi cả hai dự đoán được bao gồm trong **mod_all**, phân phối sau cho b_shark di chuyển xuống 0, trong khi đó phân phối b_temp về cơ bản vẫn giống nhau. Do đó, mối quan hệ giữa doanh số bán kem và các cuộc tấn công cá mập là một mối tương quan giả, vì cả hai đều được thông báo bởi nhiệt độ.

```{r}
bind_rows(
  spread_draws(mod_t, b_temp) %>% 
    mutate(model =  "mod_t"),
  spread_draws(mod_s, b_shark) %>% 
    mutate(model = "mod_s"),
  spread_draws(mod_all, b_temp, b_shark) %>% 
    mutate(model = "mod_all")
) %>% 
  pivot_longer(cols = starts_with("b_"), names_to = "parameter",
               values_to = "value") %>% 
  drop_na(value) %>% 
  mutate(model = factor(model, levels = c("mod_t", "mod_s", "mod_all")),
         parameter = factor(parameter, levels = c("b_temp", "b_shark"))) %>% 
  ggplot(aes(x = value, y = fct_rev(model))) +
  facet_wrap(~ parameter, nrow = 1) +
  stat_halfeye(.width = 0.89) +
  labs(x = "Parameter Value", y = "Model")


```

## **2**

Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another.

mod_i , mod_d , mod_test

```{r}
set.seed(2020)
n <- 100
u <- rnorm(n)
days_away <- rnorm(n, u)
instruction <- rnorm(n, u)
test_score <- rnorm(n, instruction - days_away)

mask_exp <- tibble(test_score, instruction, days_away) %>% 
  mutate(across(everything(), standardize))

mod_i <- brm(test_score ~ 1 + instruction, data = mask_exp, family = gaussian,
             prior = c(prior(normal(0, 0.2), class = Intercept),
                       prior(normal(0, 0.5), class = b),
                       prior(exponential(1), class = sigma)),
             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)

mod_d <- brm(test_score ~ 1 + days_away, data = mask_exp, family = gaussian,
             prior = c(prior(normal(0, 0.2), class = Intercept),
                       prior(normal(0, 0.5), class = b),
                       prior(exponential(1), class = sigma)),
             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)

mod_test <- brm(test_score ~ 1 + days_away + instruction, data = mask_exp, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)
```

```{r}
bind_rows(
  spread_draws(mod_i, b_instruction) %>% 
    mutate(model = "mod_i"),
  spread_draws(mod_d, b_days_away) %>% 
    mutate(model = "mod_d"),
  spread_draws(mod_test, b_days_away, b_instruction) %>% 
    mutate(model = "mod_test")
) %>% 
  pivot_longer(cols = starts_with("b_"), 
               names_to = "parameter", 
               values_to = "value") %>% 
  drop_na(value) %>% 
  mutate(model = factor(model, levels = c("mod_i", "mod_d", "mod_test")),
         parameter = factor(parameter, levels = c("b_instruction", "b_days_away"))) %>%
  ggplot(aes(x = value, y = fct_rev(model))) +
  facet_wrap(~ parameter, nrow = 1) +
  stat_halfeye(.width = 0.89) +
  labs(x = "Parameter Value", 
       y = "Model")
```

## **3**

It is sometimes observed that the best predictor of fire risk is the presence of firefighters—State and localities with many firefighters also have more fires. Presumably firefighters do not cause fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the same reversal of causal inference in the context of the divorce and marriage data. How might a high divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using multiple regression? 

[Đôi khi người ta quan sát thấy rằng yếu tố dự báo tốt nhất về rủi ro hỏa hoạn là sự hiện diện của lính cứu hỏa, bang và địa phương với nhiều lính cứu hỏa cũng có nhiều vụ cháy hơn. Có lẽ lính cứu hỏa không gây ra hỏa hoạn. Tuy nhiên, đây không phải là một mối tương quan giả. Thay vào đó, hỏa hoạn gây ra lính cứu hỏa. Hãy xem xét sự đảo ngược tương tự của suy luận nguyên nhân trong bối cảnh dữ liệu ly hôn và kết hôn. Làm thế nào một tỷ lệ ly hôn cao có thể gây ra tỷ lệ kết hôn cao hơn? Bạn có thể nghĩ ra một cách để đánh giá mối quan hệ này, sử dụng nhiều hồi quy?]

A high divorce rate means that there are more people available in the population of single-people that are available to marry. Additionally, people may be getting divorced for the specific purpose of marrying someone else. To evaluate this, we could add marriage number, or a “re-marry” indicator. We would then expect the coefficient for marriage rate to get closer to zero once this predictor is added to the model. 

[Tỷ lệ ly hôn cao có nghĩa là có nhiều người có sẵn trong dân số của một người có sẵn để kết hôn. Ngoài ra, mọi người có thể đã ly hôn với mục đích cụ thể là kết hôn với người khác. Để đánh giá điều này, chúng tôi có thể thêm số kết hôn hoặc chỉ báo "tái hôn". Sau đó, chúng tôi sẽ hy vọng hệ số cho tỷ lệ kết hôn sẽ tiến gần hơn đến 0 sau khi dự đoán này được thêm vào mô hình.]

## **4** 

In the divorce data, States with high numbers of members of the Church of Jesus Christ of Latter-day Saints (LDS) have much lower divorce rates than the regression models expected. Find a list of LDS population by State and use those numbers as a predictor variable, predicting divorce rate using marriage rate, median age at marriage, and percent LDS population (possibly standardized). You may want to consider transformations of the raw percent LDS variable.

[Trong dữ liệu ly hôn, các quốc gia có số lượng lớn các thành viên của Giáo hội Chúa Giêsu Kitô của các Thánh Hate-Day (LDS) có tỷ lệ ly hôn thấp hơn nhiều so với các mô hình hồi quy dự kiến. Tìm danh sách dân số LDS theo tiểu bang và sử dụng các con số đó làm biến dự đoán, dự đoán tỷ lệ ly hôn bằng tỷ lệ kết hôn, tuổi trung bình khi kết hôn và dân số LDS phần trăm (có thể là tiêu chuẩn hóa). Bạn có thể muốn xem xét các phép biến đổi của biến LDS phần trăm thô.]

```{r}
lds <- read_csv("https://raw.githubusercontent.com/wjakethompson/sr2-solutions/main/data/lds-data-2021.csv", 
                col_types = cols(.default = col_integer(),
                                 state = col_character())) %>% 
  mutate(lds_prop = members / population,
         lds_per_capita = lds_prop * 100000)

data("WaffleDivorce")
lds_divorce <- WaffleDivorce %>% 
  as_tibble() %>% 
  select(Location, Divorce, Marriage, MedianAgeMarriage) %>% 
  left_join(select(lds, state, lds_per_capita),
            by = c("Location" = "state")) %>% 
  mutate(lds_per_capita = log(lds_per_capita)) %>% 
  mutate(across(where(is.numeric), standardize)) %>% 
  filter(!is.na(lds_per_capita))

lds_mar <- brm(Divorce ~ 1 + Marriage,
               data = lds_divorce, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b, coef = Marriage),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)

lds_med <- brm(Divorce ~ 1 + MedianAgeMarriage, 
               data = lds_divorce, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b, coef = MedianAgeMarriage),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)

lds_cap <- brm(Divorce ~ 1 + lds_per_capita,
               data = lds_divorce, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b, coef = lds_per_capita),
                         prior(exponential(1), class = sigma)),
               iter= 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)

lds_mod <- brm(Divorce ~ 1 + Marriage + MedianAgeMarriage + lds_per_capita,
               data = lds_divorce, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b, coef = Marriage),
                         prior(normal(0, 0.5), class = b, coef = MedianAgeMarriage),
                         prior(normal(0, 0.5), class = b, coef = lds_per_capita),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)

bind_rows(
  spread_draws(lds_mar, `b_.*`, regex = TRUE) %>% 
    mutate(model = "lds_mar"),
  spread_draws(lds_med, `b_.*`, regex = TRUE) %>% 
    mutate(model = "lds_med"),
  spread_draws(lds_cap, `b_.*`, regex = TRUE) %>% 
    mutate(model = "lds_cap"),
  spread_draws(lds_mod, `b_.*`, regex = TRUE) %>% 
    mutate(model = "lds_mod")
) %>% 
  pivot_longer(starts_with("b_"),
               names_to = "parameter",
               values_to = "value",
               values_drop_na = TRUE) %>% 
  mutate(model = factor(model, levels = c("lds_mar", "lds_med", "lds_cap", "lds_mod")),
         parameter = factor(parameter, levels = c("b_Intercept", "b_Marriage", "b_MedianAgeMarriage", "b_lds_per_capita"))) %>% 
  ggplot(aes(x = value, y = fct_rev(model))) +
  facet_wrap(~ parameter, nrow = 2) +
  stat_halfeye(.width = 0.89) +
  labs(x = "Parameter Value", 
       y = "Model")

# 
spread_draws(lds_mod, `b_.*`, regex = TRUE) %>% 
  pivot_longer(starts_with("b_"), 
               names_to = "parameter",
               values_to = "value") %>% 
  ggplot(aes(x = value, y = parameter)) +
  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +
  geom_vline(xintercept = 0, color = "red") +
  labs(x = "Parameter Value",
       y = "Parameter")

```

# **Hard**

## **1**

In the divorce example, suppose the DAG is: M → A → D. What are the implied conditional independencies of the graph? Are the data consistent with it?

```{r}
library(dagitty)

mad_dag <- dagitty("dag{M -> A -> D}")
impliedConditionalIndependencies(mad_dag)

equivalentDAGs(mad_dag)
```

   We can check these with the data, provided we are willing to make some additional statistical assumptions about the functions that relate each variable to the others. The only functions we’ve used so far in the book are linear (additive) functions. The implication above suggests that a regression of D on both M and A should show little association between D and M. You know from the chapter that this is true in the divorce data sample.
   
   So the data are consistent with this graph. But can you think of a way that marriage rate `M` would causally influence median age of marriage `A`? If you cannot, then maybe this graph fails on basic scientific grounds. No data are required.

## **2**

Assuming that the DAG for the divorce example is indeed  M → A → D, fit a new model and use it to estimate the counterfactual effect of halving a State’s marriage rate M. Using the counterfactual example from the chapter (starting on page 140) as a template.

```{r}
dat <- WaffleDivorce %>%
  select(A = MedianAgeMarriage,
         D = Divorce,
         M = Marriage) %>%
  mutate(across(everything(), standardize))

d_model <- bf(D ~ 1 + A)
a_model <- bf(A ~ 1 + M)

b5h2 <- brm(d_model + a_model + set_rescor(FALSE),
            data = dat, family = gaussian,
            prior = c(prior(normal(0, 0.2), class = Intercept, resp = D),
                      prior(normal(0, 0.5), class = b, resp = D),
                      prior(exponential(1), class = sigma, resp = D),
                      
                      prior(normal(0, 0.2), class = Intercept, resp = A),
                      prior(normal(0, 0.5), class = b, resp = A),
                      prior(exponential(1), class = sigma, resp = A)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp5", "b5h2"))

summary(b5h2)
#>  Family: MV(gaussian, gaussian) 
#>   Links: mu = identity; sigma = identity
#>          mu = identity; sigma = identity 
#> Formula: D ~ 1 + A 
#>          A ~ 1 + M 
#>    Data: dat (Number of observations: 50) 
#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;
#>          total post-warmup draws = 8000
#> 
#> Population-Level Effects: 
#>             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> D_Intercept     0.00      0.10    -0.20     0.20 1.00    10726     6132
#> A_Intercept    -0.00      0.09    -0.17     0.18 1.00    10190     6073
#> D_A            -0.56      0.11    -0.78    -0.34 1.00     9422     6605
#> A_M            -0.69      0.10    -0.89    -0.49 1.00     8993     5538
#> 
#> Family Specific Parameters: 
#>         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> sigma_D     0.82      0.09     0.67     1.01 1.00    10660     5820
#> sigma_A     0.71      0.07     0.58     0.88 1.00    10351     6034
#> 
#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
#> and Tail_ESS are effective sample size measures, and Rhat is the potential
#> scale reduction factor on split chains (at convergence, Rhat = 1).
```

```{r}
as_draws_df(b5h2) %>%
  as_tibble() %>%
  select(.draw, b_D_Intercept:sigma_A) %>% 
  expand(nesting(.draw, b_D_Intercept, b_A_Intercept, b_D_A, b_A_M,
                 sigma_D, sigma_A),
         m = seq(from = -2, to = 2, length.out = 30)) %>%
  mutate(a_sim = rnorm(n(), mean = b_A_Intercept + b_A_M * m, sd = sigma_A),
         d_sim = rnorm(n(), mean = b_D_Intercept + b_D_A * a_sim, sd = sigma_D)) %>%
  pivot_longer(ends_with("_sim"), names_to = "name", values_to = "value") %>%
  group_by(m, name) %>%
  mean_qi(value, .width = c(0.89)) %>%
  ungroup() %>%
  mutate(name = case_when(name == "a_sim" ~ "Counterfactual M &rarr; A",
                          TRUE ~ "Counterfactual M &rarr; A &rarr; D")) %>%
  ggplot(aes(x = m, y = value, ymin = .lower, ymax = .upper)) +
  facet_wrap(~name, nrow = 1) +
  geom_smooth(stat = "identity") +
  labs(x = "Manipulated M", y = "Counterfactual")
```

```{r}
as_draws_df(b5h2) %>% 
  mutate(a_avg = rnorm(n(), b_A_Intercept + b_A_M * 0, sigma_A),
         a_hlf = rnorm(n(), b_A_Intercept + b_A_M * -2.66, sigma_A),
         d_avg = rnorm(n(), b_D_Intercept + b_D_A * a_avg, sigma_D),
         d_hlf = rnorm(n(), b_D_Intercept + b_D_A * a_hlf, sigma_D),
         diff = d_hlf - d_avg) %>% 
  mean_hdi(diff, .width = 0.89)
```

---

### **2nd solution**

   The first thing to do is outline the full statistical model. If the DAG is `M → A → D`, then this implies two regressions. The first regresses A on M and the second D on A. That is the model we need to program, in order to compute the counterfactual prediction of intervening on M. The model contains both regressions, and the estimates from both regressions are used to compute the counterfactual prediction, because any intervention on M first influences A which then influences D.

   Start by loading the data:
```{r}
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce
d$D <- standardize( d$Divorce )
d$M <- standardize( d$Marriage )
d$A <- standardize( d$MedianAgeMarriage )
```
   Now the model:
```{r}
m5H2 <- quap(
  alist(
    # A -> D
    D ~ dnorm( muD , sigmaD ),
    muD <- aD + bAD*A,
    # M -> A
    A ~ dnorm( muA , sigmaA ),
    muA <- aA + bMA*M,
    # priors
    c(aD,aA) ~ dnorm(0,0.2),
    c(bAD,bMA) ~ dnorm(0,0.5),
    c(sigmaD,sigmaA) ~ dexp(1)
  ) , data=d )
precis(m5H2)

#        mean sd    5.5%  94.5%
# aD     0.00 0.10 -0.16  0.16
# aA     0.00 0.09 -0.14  0.14
# bAD   -0.57 0.11 -0.74 -0.39
# bMA   -0.69 0.10 -0.85 -0.54
# sigmaD 0.79 0.08  0.66  0.91
# sigmaA 0.68 0.07  0.57  0.79
```

   This just shows what you already know from the chapter: A and M are negatively associated and A and D are also negatively associated. It’s interpreting these associations as having causal directions that makes counterfactual prediction.

   Now the counterfactual prediction is accomplished by building an input range of values for the interventions on M and then simulating both A and then D. We can use `sim` for this. Or you could do it in raw code just with rnorm and samples from the posterior. Here is the `sim` approach, just like in the chapter:

```{r}
M_seq <- seq( from=-3 , to=3 , length.out=30 )

sim_dat <- data.frame( M=M_seq )

s <- sim( m5H2 , data=sim_dat , vars=c("A","D") )

plot( sim_dat$M , colMeans(s$A) , ylim=c(-2,2) , type="l" ,
      xlab="manipulated M" , ylab="counterfactual A" )
shade( apply(s$A,2,PI) , sim_dat$M )
mtext( "Counterfactual M -> A" )

plot( sim_dat$M , colMeans(s$D) , ylim=c(-2,2) , type="l" , 
      xlab="manipulated M" , ylab="counterfactual D" )
shade( apply(s$D,2,PI) , sim_dat$M )
mtext( "Counterfactual M -> A -> D" )
```

   The left plot shows the intermediate `M → A` effect, which comes directly from the coefficient bMA. This effect is negative, with one unit change in M moving A by about −0.7 on average. The right plot shows the full counterfactual effect of M on D. This effect depends upon both coefficients bMA and bAD. It is not negative, but rather positive. Why? Because both coefficients are negative. If increasing A decreases D, then decreasing A increases D. Since increasing M decreases A, it follows that increasing M increases D. If that is confusing, that’s natural. Luckily the model can get this right without even understanding it.

   In simple linear models like these, the total causal effect along a path like `M → M → D` is given by multiplying the coefficients along the path. In this case that means the product of `bMA` and `bAD`. So that’s (−0.57) × (−0.69) ≈ 0.4 at the posterior mean.

   Okay, but the question asked what would happen if we halved a State’s marriage rate. And this is the hard part of the problem. We have to decide what the original marriage rate was, to know what halving it means. And then we have to convert to the standardized scale that the model uses

   The average marriage rate in the sample is about 20. So if we halve the rate of an average State, it ends up at 10. What is 10 in the standardized units of the model? We find out by standardizing it!

```{r}
(10 - mean(d$Marriage))/sd(d$Marriage)
```
[1] -2.663047

   That would be a huge effect! Looking at the righthand plot above, going from M = 0 to M = −2.7 would take us from about D = 0 to D = −1. Or we could do this calculation more directly:

```{r}
M_seq <- c( 0 , -2.67 )
sim_dat <- data.frame( M=M_seq )
s <- sim( m5H2 , data=sim_dat , vars=c("A","D") )
diff <- s$D[,2] - s$D[,1]
mean( diff )
```
[1] -1.050188

   So the causal effect of halving an average State’s marriage rate is to decrease divorce by a full standard deviation. Well, if this causal model is right.


## **3**

```{r}
data(milk)

new_milk <- milk %>%
  select(kcal.per.g,
         neocortex.perc,
         mass) %>%
  drop_na(everything()) %>%
  mutate(log_mass = log(mass),
         K = standardize(kcal.per.g),
         N = standardize(neocortex.perc),
         M = standardize(log_mass))

K_model <- bf(K ~ 1 + M + N)
N_model <- bf(N ~ 1 + M)

b5h3 <- brm(K_model + N_model + set_rescor(FALSE),
            data = new_milk, family = gaussian,
            prior = c(prior(normal(0, 0.2), class = Intercept, resp = K),
                      prior(normal(0, 0.5), class = b, resp = K),
                      prior(exponential(1), class = sigma, resp = K),
                      
                      prior(normal(0, 0.2), class = Intercept, resp = N),
                      prior(normal(0, 0.5), class = b, resp = N),
                      prior(exponential(1), class = sigma, resp = N)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp5", "b5h3"))

summary(b5h3)
#>  Family: MV(gaussian, gaussian) 
#>   Links: mu = identity; sigma = identity
#>          mu = identity; sigma = identity 
#> Formula: K ~ 1 + M + N 
#>          N ~ 1 + M 
#>    Data: new_milk (Number of observations: 17) 
#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;
#>          total post-warmup draws = 8000
#> 
#> Population-Level Effects: 
#>             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> K_Intercept     0.00      0.14    -0.27     0.28 1.00     8841     5684
#> N_Intercept     0.00      0.13    -0.25     0.26 1.00     9335     5242
#> K_M            -0.68      0.27    -1.17    -0.11 1.00     5097     5278
#> K_N             0.57      0.26     0.02     1.05 1.00     4924     5189
#> N_M             0.66      0.17     0.32     0.98 1.00     8326     5454
#> 
#> Family Specific Parameters: 
#>         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> sigma_K     0.81      0.17     0.56     1.21 1.00     6085     5711
#> sigma_N     0.72      0.14     0.50     1.05 1.00     8236     5409
#> 
#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
#> and Tail_ESS are effective sample size measures, and Rhat is the potential
#> scale reduction factor on split chains (at convergence, Rhat = 1).
```


```{r}
milk_cf <- as_draws_df(b5h3) %>%
  as_tibble() %>% 
  select(.draw, b_K_Intercept:sigma_N) %>% 
  expand(nesting(.draw, b_K_Intercept, b_N_Intercept, b_K_M, b_K_N, b_N_M,
                 sigma_K, sigma_N),
         mass = seq(from = 0.5, to = 80, by = 0.5)) %>%
  mutate(log_mass = log(mass),
         M = (log_mass - mean(new_milk$log_mass)) / sd(new_milk$log_mass),
         n_sim = rnorm(n(), mean = b_N_Intercept + b_N_M * M, sd = sigma_N),
         k_sim = rnorm(n(), mean = b_K_Intercept + b_K_N * n_sim + b_K_M * M,
                       sd = sigma_K)) %>%
  pivot_longer(ends_with("_sim"), names_to = "name", values_to = "value") %>%
  group_by(mass, name) %>%
  mean_qi(value, .width = c(0.89)) %>%
  ungroup() %>%
  filter(name == "k_sim") %>%
  mutate(name = case_when(name == "n_sim" ~ "Counterfactual effect M on N",
                          TRUE ~ "Total Counterfactual effect of M on K"))

ggplot(milk_cf, aes(x = mass, y = value, ymin = .lower, ymax = .upper)) +
  geom_smooth(stat = "identity") +
  labs(x = "Manipulated Mass", y = "Counterfactual K")
```

```{r}
(log(c(15, 30)) - mean(log(milk$mass))) / sd(log(milk$mass)) 
#> [1] 0.746 1.154

as_draws_df(b5h3) %>% 
  mutate(n_avg = rnorm(n(), b_N_Intercept + b_N_M * 0.746, sigma_N),
         n_dbl = rnorm(n(), b_N_Intercept + b_N_M * 1.154, sigma_N),
         k_avg = rnorm(n(), b_K_Intercept + b_K_M * 0.746 + b_K_N * n_avg,
                       sigma_K),
         k_dbl = rnorm(n(), b_K_Intercept + b_K_M * 1.154 + b_K_N * n_dbl,
                       sigma_K),
         diff = k_dbl - k_avg) %>% 
  median_hdi(diff, .width = 0.89)
#> # A tibble: 1 × 6
#>     diff .lower .upper .width .point .interval
#>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    
#> 1 -0.151  -2.45   1.82   0.89 median hdi
```

```{r}
library(dagitty)

mad_dag <- dagitty("dag{M -> A -> D}")
impliedConditionalIndependencies(mad_dag)

smoke_dag <- dagitty("dag{X -> Y <- A -> S}")
impliedConditionalIndependencies(smoke_dag)
```

## **4**

Here is an open practice problem to engage your imagination. In the divorce data, States in the southern United States have many of the highest divorce rates. Add the South indicator variable to the analysis. First, draw one or more DAGs that represent your ideas for how Southern American culture might influence any of the other three variables ( D, M, or A). Then list the testable implications of your DAGs, if there are any, and fit one or more models to evaluate the implications. What do you think the influence of “Southerness” is?

```{r}
library(ggdag)

dag_coords <-
  tibble(name = c("S", "A", "M", "D"),
         x = c(1, 1, 2, 3),
         y = c(3, 1, 2, 1))

dagify(D ~ A + M,
       M ~ A + S,
       A ~ S,
       coords = dag_coords) %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_text(color = "black", size = 10) +
  geom_dag_edges(edge_color = "black", edge_width = 2,
                 arrow_directed = grid::arrow(length = grid::unit(15, "pt"),
                                              type = "closed")) +
  theme_void()

div_dag <- dagitty("dag{S -> M -> D; S -> A -> D; A -> M}")
impliedConditionalIndependencies(div_dag)
#> D _||_ S | A, M
```

   Given the DAG, divorce rate should be independent of southerness when we condition on median age at marriage and marriage rate. Let’s estimate a model to test this. We’ll include median age at marriage (A), marriage rate (M), and southerness (S) in the model. Because both A and M are included, we would expect the coefficient for S to be zero, if the implied conditional independency holds.

```{r}
data("WaffleDivorce")

south_divorce <- WaffleDivorce %>%
  as_tibble() %>%
  select(D = Divorce,
         A = MedianAgeMarriage,
         M = Marriage,
         S = South) %>%
  drop_na(everything()) %>%
  mutate(across(where(is.double), standardize))

b5h4 <- brm(D ~ A + M + S, data = south_divorce, family = gaussian,
            prior = c(prior(normal(0, 0.2), class = Intercept),
                      prior(normal(0, 0.5), class = b),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp5", "b5h4"))

spread_draws(b5h4, b_S) %>%
  ggplot(aes(x = b_S)) +
  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +
  labs(x = expression(beta[S]), y = "Density")
```




