---
title: "Untitled"
author: "Hoang-Chau Luong"
date: '2022-09-12'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rethinking)
library(tidybayes)
library(brms)

theme_set(theme_light())
```

# **Easy**

## **1**

State the three motivating criteria that define information entropy. Try to express each in your own words.

**Answer**

Insight of information entropy is "How much is our uncertainty reduced by learning an outcome? 3 criteria:

1. The measure of uncertainty should be continuous
2. The measure of uncertainty should increase as the number of possible events increases -> Sample set larger -> uncertainty increases
3. The measure of uncertainty should be additive

## **2**

:::quesion
>Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?
:::

**Answer**
```{r}
p_logp <- function(p){
  if(p == 0) return(0)
  p * log(p)
}

calc_entropy <- function(x){
  avg_logprob <- sum(map_dbl(x, p_logp))
  -1 * avg_logprob
}

probs <- c(0.7, 0.3)

calc_entropy(probs)
```

## **3**

:::question
> Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die?
:::

**Answer**
```{r}
probs <- c(0.2, 0.25, 0.25, 0.3)
calc_entropy(probs)
```

## **4**

:::question
>Suppose another four-sided die is loaded such that it never shows “4.” The other three sides show equally often. What is the entropy of this die
:::

**Answer**
```{r}
probs <- c(1/3, 1/3, 1/3, 0)
calc_entropy(probs)
```

# **Medium**

## **1**

:::question
>Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criterion into a less general one?
:::

**Answer**
$$
\text{lppd} = \sum_{i}^{n}\text{log}
\int\text{p}(\text{y}_{i}|\theta)\text{p}_{post}(\theta)d\theta
$$

$$
\text{AIC} = \text{-2*llpd + 2p} \\
WAIC(y, \theta) = \text{-2(lppd - }\sum_{i}\text{var}_{\theta}\text{log}p(y_{i}|\theta))
$$

The WAIC is more general than the AIC, as the AIC assumes that priors are flat or overwhelmed by the likelihood, the posterior distribution is approximately multivariate Gaussian, and the sample size is much greater than the number of parameters. If all of these assumptions are met, then we would expect the AIC and WAIC to be about the same. 

## **2**

:::question
>Explain the difference between model selection and model comparison. What information is lost under model selection?
:::

**Answer**
   Model selection refers to just pick the model that has the lowest criterion value and discarding other models. When we take this approach, we lose information about the relative model accuracy that can inform how confident we are in the models
   Additionally, model selection only cares about predictive accuracy and ignores causal inferences. Thus, a model may be selected that has confounds or that would not correctly inform an intervention
   
   In contrast, model comparison uses multiple models to understand about how the variables included influence prediction and affect implied conditional independencies in a causal model
   -> Thus, we can preserve information and can make more hoslistic judgement about the data & models.
   
## **3**

:::question
>When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.
:::

**Answer**
  If we compare models which is trained on different datasets, then we will get into the inconsistency state. The underlying of comparing something is the same context, the same treatment.

  All of the information criteria are defined based on the log-pointwise-predictive density, defined as follows, where y is the data, $\theta$ is the posterior distribution, S is the number of samples, and I is the number of samples.
  
$$
lppd(y,\theta) = \sum_{i}\text{log}\frac{1}{S}\sum_{s}p(y_{i}|\theta_{s})
$$

  In the words, this means take the log of the average probability across samples of each observation i and sum them together. Thus a larger sample size will necessarily lead to a smaller log-pointwise-predictive-density, even if the data generating process and models are exactly equivalent (i.e., when the lppd values are negative, the sum will get more negative as the sample size increases). More observations are entered into the sum, leading a smaller final lppd, which will in turn increase the information criteria. We can run a quick simulation to demonstrate. For three different sample sizes, we'll simulate 100 data sets from the exact same data generation process, estimate a exact same linear model, and then calculate the lppd, WAIC, and PSIS for each.
  
```{r}
gen_data <- function(n){
  tibble(x1 = rnorm(n)) %>% 
    mutate(y = rnorm(n = n, mean = 0.3 + 0.8 * x1),
           across(everything(), standardize))
}

fit_model <- function(dat) {
  suppressMessages(output <- capture.output(
    mod <- brm(y ~ 1 + x1, data = dat, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = "b"),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123)
  ))
  
  return(mod)
}


calc_info <- function(model) {
  mod_lppd <- log_lik(model) %>% 
    as_tibble(.name_repair = "minimal") %>% 
    set_names(paste0("obs_", 1:ncol(.))) %>% 
    rowid_to_column(var = "iter") %>% 
    pivot_longer(-iter, names_to = "obs", values_to = "logprob") %>% 
    mutate(prob = exp(logprob)) %>% 
    group_by(obs) %>% 
    summarize(log_prob_score = log(mean(prob))) %>% 
    pull(log_prob_score) %>% 
    sum()
  
  mod_waic <- suppressWarnings(waic(model)$estimates["waic", "Estimate"])
  mod_psis <- suppressWarnings(loo(model)$estimates["looic", "Estimate"])
  
  tibble(lppd = mod_lppd, waic = mod_waic, psis = mod_psis)
}

sample_sim <- tibble(sample_size = rep(c(100, 500, 1000), each = 100)) %>%
  mutate(sample_data = purrr::map(sample_size, gen_data),
         model = purrr::map(sample_data, fit_model),
         infc = purrr::map(model, calc_info)) %>%
  select(-sample_data, -model) %>% 
  unnest(infc) 
```

```{r}
library(ggridges)

sample_sim %>%
  pivot_longer(cols = c(lppd, waic, psis)) %>%
  mutate(sample_size = fct_inorder(as.character(sample_size)),
         name = str_to_upper(name),
         name = fct_inorder(name)) %>%
  ggplot(aes(x = value, y = sample_size)) +
  facet_wrap(~name, nrow = 1, scales = "free_x") +
  geom_density_ridges(bandwidth = 4) +
  scale_y_discrete(expand = c(0, .1)) +
  scale_x_continuous(breaks = seq(-2000, 3000, by = 750)) +
  coord_cartesian(clip = "off") +
  labs(x = "Value", y = "Sample Size")
```
  

## **4**

:::question
>What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.
:::

**Answer**
$$
\text{lppd} = \sum_{i}^{n}\text{log}
\int\text{p}(\text{y}_{i}|\theta)\text{p}_{post}(\theta)d\theta
$$

$$
\text{AIC} = \text{-2*llpd + 2p} \\
WAIC(y, \theta) = \text{-2(lppd - }\sum_{i}\text{var}_{\theta}\text{log}p(y_{i}|\theta))
$$

  Smaller variances in log probabilities will results in a lower penalty. If we restrict the prior to become more concentrated, we restrict the plausible range of the parameters. In other words, we restrict the variability in the posterior distribution. As the parameters become more consistent, the log probability of each observation will necessarily become more consistent also. Thus, the penalty term, or effective number of parameters, becomes smaller. We can again confirm with a small simulation


## **5**

:::question
>Provide an informal explanation of why informative priors reduce overfitting.
:::

**Answer**
   Informative priors restrict the plausible values for parameters. By using informative priors, we can limit the values of parameters to values that are reasonable, given our scientific knowledge. Thus, we can keep the model from learning too much from our specific sample.


## **6**

:::question
> Provide an informal explanation of why overly informative priors result in underfitting.
:::

**Answer**
   In contrast to the previous question, making the prior too informative can be too restrictive on the parameter space. This prevents our model from learning enough from our sample. We basically just get our prior distributions back, without learning anything from the data that could help make future predictions.

# **Hard**

## **1**

:::question
>In 2007, The Wall Street Journal published an editorial (“We’re Number One, Alas”) with a graph of corportate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in (reconstructed at right), seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. I want you to actually fit a curve to these data, found in `data(Laffer)`. Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue.
:::

```{r}
data(Laffer)
df <- Laffer

df %>% 
  as_tibble() %>% 
  ggplot(aes(x = tax_rate, y = tax_revenue)) +
  geom_point()

df <- df %>% 
  mutate(across(everything(), standardize),
         tax_rate2 = tax_rate^2)

laf_line <- brm(tax_revenue ~ 1 + tax_rate, 
                data = df, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 1000, warmup = 500, chains = 4, cores = 4, seed = 123,
                file = here::here("fits", "chp7", "b7h1-line.rds"))

laf_quad <- brm(tax_revenue ~ 1 + tax_rate + tax_rate2, 
                data = df, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 1000, warmup = 500, chains = 4, cores = 4, seed = 123,
                file = here::here("fits", "chp7", "b7h1-quad.rds"))
# spine
laf_spln <- brm(tax_revenue ~ 1 + s(tax_rate, bs = "bs"),
                data = df, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(normal(0, 0.5), class = sds),
                          prior(exponential(1), class = sigma)),
                iter = 1000, warmup = 500, chains = 4, cores = 4, seed = 123,
                control = list(adapt_delta = 0.95),
                file = here::here("fits", "chp7", "b7h1-spln.rds"))

laf_line <- readRDS(here::here("fits", "chp7", "b7h1-line.rds"))
laf_quad <- readRDS(here::here("fits", "chp7", "b7h1-quad.rds"))
laf_spln <- readRDS(here::here("fits", "chp7", "b7h1-spln.rds"))
```

```{r}
library(wjake)

tr_seq <- tibble(tax_rate = seq(0, 40, length.out = 100)) %>% 
  mutate(tax_rate = (tax_rate - mean(Laffer$tax_rate)) / sd(Laffer$tax_rate),
         tax_rate2 = tax_rate^2)

predictions <- bind_rows(
  predicted_draws(laf_line, newdata = tr_seq) %>% 
    median_qi(.width = 0.89) %>% 
    mutate(type = "Linear"),
  predicted_draws(laf_quad, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Quadratic"),
  predicted_draws(laf_spln, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Spline")
)

fits <- bind_rows(
  epred_draws(laf_line, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Linear"),
  epred_draws(laf_quad, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Quadratic"),
  epred_draws(laf_spln, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Spline")
)

ggplot() +
  facet_wrap(~type, nrow = 1) +
  geom_ribbon(data = predictions,
              aes(x = tax_rate, ymin = .lower, ymax = .upper),
              alpha = 0.2) +
  geom_lineribbon(data = fits,
                  aes(x = tax_rate, y = .epred, ymin = .lower, ymax = .upper),
                  size = 0.6) +
  geom_point(data = df, aes(x = tax_rate, y = tax_revenue),
             alpha = 0.5) +
  scale_fill_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                    breaks = c(0.67, 0.89, 0.97)) +
  labs(x = "Standardized Tax Rate", y = "Standardized Tax Revenue",
       fill = "Interval")
```

They all look pretty similar, but the quadratic and spline models do show a slight curve. Next, we can look at the PSIS (called LOO in {brms} and {rstan}) and WAIC comparisons. Neither the PSIS or WAIC is really able to differentiate the models in a meaningful way. However, it should be noted that both the PSIS and WAIC have Pareto or penalty values that are exceptionally large, which could make the criteria unreliable.

```{r}
library(loo)

laf_line <- add_criterion(laf_line, criterion = c("loo", "waic"))
laf_quad <- add_criterion(laf_quad, criterion = c("loo", "waic"))
laf_spln <- add_criterion(laf_spln, criterion = c("loo", "waic"))

loo_compare(laf_line, laf_quad, laf_spln, criterion = "waic")
#>          elpd_diff se_diff
#> laf_quad  0.0       0.0   
#> laf_spln -0.1       0.6   
#> laf_line -0.9       0.9
loo_compare(laf_line, laf_quad, laf_spln, criterion = "loo")
#>          elpd_diff se_diff
#> laf_spln  0.0       0.0   
#> laf_quad  0.0       0.7   
#> laf_line -0.8       0.9
```


## **2**

:::question
>In the Laffer data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Student’s t distribution to revist the curve fitting problem. How much does a curved relationship depend upon the outlier point.
:::

```{r}
library(gghighlight)

criteria_influence <- function(mod) {
  tibble(pareto_k = mod$criteria$loo$diagnostics$pareto_k,
         p_waic = mod$criteria$waic$pointwise[, "p_waic"]) %>% 
    rowid_to_column(var = "obs")
}

influ <- bind_rows(
  criteria_influence(laf_line) %>% 
    mutate(type = "Linear"),
  criteria_influence(laf_quad) %>% 
    mutate(type = "Quadratic"),
  criteria_influence(laf_spln) %>% 
    mutate(type = "Spline")
)

influ %>% 
  ggplot(aes(x = pareto_k, y = p_waic)) +
  facet_wrap(~ type, nrow = 1) +
  geom_vline(xintercept = 0.7, linetype = "dashed") +
  geom_hline(yintercept = 0.4, linetype = "dashed") +
  geom_point() +
  gghighlight(pareto_k > 0.7 | p_waic > 0.4, n = 1, label_key = obs,
              label_params = list(size = 3)) +
  labs(x = "Pareto *k*", y = "p<sub>WAIC</sub>")
```

Let’s refit the model using a Student’s t distribution to put larger tails on our outcome distribution, and then visualize our new models.

```{r}
laf_line2 <- brm(bf(tax_revenue ~ 1 + tax_rate, nu = 1),
                 data = df, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(exponential(1), class = sigma)),
                 iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
                 file = here::here("fits", "chp7", "b7h2-line.rds"))

laf_quad2 <- brm(bf(tax_revenue ~ 1 + tax_rate + tax_rate2, nu = 1),
                 data = df, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(exponential(1), class = sigma)),
                 iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
                 file = here::here("fits", "chp7", "b7h2-quad.rds"))

laf_spln2 <- brm(bf(tax_revenue ~ 1 + s(tax_rate, bs = "bs"), nu = 1),
                 data = df, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(exponential(1), class=  sigma)),
                 iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
                 file = here::here("fits", "chp7", "b7h2-spln.rds"))

predictions <- bind_rows(
  predicted_draws(laf_line2, newdata = tr_seq) %>% 
    median_qi(.width = 0.89) %>% 
    mutate(type = "Linear"),
  predicted_draws(laf_quad2, newdata = tr_seq) %>% 
    median_qi(.width = 0.89) %>% 
    mutate(type = "Quadratic"),
  predicted_draws(laf_spln2, newdata = tr_seq) %>% 
    median_qi(.width = 0.89) %>% 
    mutate(type = "Spline")
)


# epred_draws -> 
fits <- bind_rows(
  add_epred_draws(laf_line2, newdata = tr_seq) %>% 
    median_qi(.width = c(0.67, 0.89, 0.97)) %>% 
    mutate(type = "Linear"),
  add_predicted_draws(laf_quad2, newdata = tr_seq) %>% 
    median_qi(.width = c(0.67, 0.89, 0.97)) %>% 
    mutate(type = "Quadratic"),
  add_predicted_draws(laf_spln2, newdata = tr_seq) %>% 
    median_qi(.width = c(0.67, 0.89, 0.97)) %>% 
    mutate(type = "Spline")
)

ggplot() +
  facet_wrap(~type, nrow = 1) +
  geom_ribbon(data = predictions, 
              aes(x = tax_rate, ymin = .lower, ymax = .upper),
              alpha = 0.2) +
  geom_lineribbon(data = fits,
                  aes(x = tax_rate, y = .epred, ymin = .lower, ymax = .upper),
                  alpha = 0.6) +
  geom_point(data = df, aes(x = tax_rate, y = tax_revenue),
             alpha = 0.5) +
  scale_fill_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                    breaks = c(0.67, 0.89, 0.97)) +
  labs(x = "Standardized Tax Rate", y = "Standardized Tax Revenue",
       fill = "Interval")
```

The prediction intervals are a little bit narrower, which makes sense as the predictions are no longer being as influenced by the outlier. When we look at the new PSIS and WAIC estimates, we are no longer getting warning messages about large Pareto k values; however, we do still see warnings about large $p_{WAIC}$ values. The comparisons also tell the same story as before, with no distinguishable differences between the models.

```{r}
laf_line2 <- add_criterion(laf_line2, criterion = c("loo", "waic"))
laf_quad2 <- add_criterion(laf_quad2, criterion = c("loo", "waic"))
laf_spln2 <- add_criterion(laf_spln2, criterion = c("loo", "waic"))

loo_compare(laf_line2, laf_quad2, laf_spln2, criterion = "waic")
#>           elpd_diff se_diff
#> laf_quad2  0.0       0.0   
#> laf_spln2 -0.3       1.3   
#> laf_line2 -1.1       1.7
loo_compare(laf_line2, laf_quad2, laf_spln2, criterion = "loo")
#>           elpd_diff se_diff
#> laf_quad2  0.0       0.0   
#> laf_spln2 -0.2       1.2   
#> laf_line2 -1.1       1.7
```

## **3**

Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species: Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. First, compute the entropy of each island’s bird distribution. Interpret these entropy values. Second, use each island’s bird distribution to predict the other two. This means to compute the KL divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different KL divergence values. Which island predicts the others best? Why?

First, lets compute the entropy for each each island.

```{r}
islands <- tibble(island = paste("Island", 1:3),
                  a = c(0.2, 0.8, 0.05),
                  b = c(0.2, 0.1, 0.15),
                  c = c(0.2, 0.05, 0.7),
                  d = c(0.2, 0.025, 0.05),
                  e = c(0.2, 0.025, 0.05)) %>%
  pivot_longer(-island, names_to = "species", values_to = "prop")

islands %>% 
  group_by(island) %>% 
  summarize(prop = list(prop), .groups = "drop") %>% 
  mutate(entropy = map_dbl(prop, calc_entropy))

#> # A tibble: 3 × 3
#>   island   prop      entropy
#>   <chr>    <list>      <dbl>
#> 1 Island 1 <dbl [5]>   1.61 
#> 2 Island 2 <dbl [5]>   0.743
#> 3 Island 3 <dbl [5]>   0.984
```

The first island has the highest entropy. This is expected, because it has the most even distribution of bird species. All species are equally likely, so the observation of any one species is not surprising. In contrast, Island 2 has the lowest entropy. This is because the vast majority of birds on this island are Species A. Therefore, observing a bird that is not from Species A would be surprising.

For the second part of the question, we need to compute the KL divergence for each pair of islands. The KL divergence is defined as:

$$
\text{D}_{KL} = \sum_{i}\text{p}_{i}(\text{log(}p_{i}) - \text{log(}q_{i}) )
$$

```{r}
d_kl <- function(p, q) {
  sum(p * (log(p) - log(q)))
}
```

```{r}
crossing(model = paste("Island", 1:3),
         predicts = paste("Island", 1:3)) %>%
  filter(model != predicts) %>%
  left_join(islands, by = c("model" = "island")) %>%
  rename(model_prop = prop) %>%
  left_join(islands, by = c("predicts" = "island", "species")) %>%
  rename(predict_prop = prop) %>%
  group_by(model, predicts) %>%
  summarize(q = list(model_prop),
            p = list(predict_prop),
            .groups = "drop") %>%
  mutate(kl_distance = map2_dbl(p, q, d_kl))
#> # A tibble: 6 × 5
#>   model    predicts q         p         kl_distance
#>   <chr>    <chr>    <list>    <list>          <dbl>
#> 1 Island 1 Island 2 <dbl [5]> <dbl [5]>       0.866
#> 2 Island 1 Island 3 <dbl [5]> <dbl [5]>       0.626
#> 3 Island 2 Island 1 <dbl [5]> <dbl [5]>       0.970
#> 4 Island 2 Island 3 <dbl [5]> <dbl [5]>       1.84 
#> 5 Island 3 Island 1 <dbl [5]> <dbl [5]>       0.639
#> 6 Island 3 Island 2 <dbl [5]> <dbl [5]>       2.01
```

These results show us that when using Island 1 to predict Island 2, the KL divergence is about *0.87*. When we use Island 1 to predict Island 3, the KL divergence is about *0.63*, and so on. Overall, the distances are shorter when we used Island 1 as the model. This is because Island 1 has the highest entropy. Thus, we are less surprised by the other islands, so there’s a shorter distance. In contrast, Island 2 and Island 3 have very concentrated distributions, so predicting the other islands leads to more surprises, and therefore greater distances.

## **4**

:::question
>Recall the marriage age, and happiness collider bias example from Chapter 6. Run model `m6.9` and `m6.10` again (page 178). Compare these two models using WAIC (or PSIS, they will produce identical resuts). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?

As a reminder, here is the DAG for this example, where ***H*** is happiness, ***M*** is marriage, and ***A*** is age.

```{r}
library(dagitty)
library(ggdag)

hma_dag <- dagitty("dag{H -> M <- A}")

coordinates(hma_dag) <- list(x = c(H = 1, M = 2, A = 3),
                             y = c(H = 1, M = 1, A = 1))

ggplot(hma_dag, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_text(color = "black", size = 10) +
  geom_dag_edges(edge_color = "black", edge_width = 2,
                 arrow_directed = grid::arrow(length = grid::unit(15, "pt"),
                                              type = "closed")) +
  theme_void()
```

First, let's regenerate the data and estimate the models.

```{r}
d <- sim_happiness(seed = 1977, N_years = 1000)
dat <- d %>% 
  filter(age > 17) %>% 
  mutate(a = (age - 18) / (65 - 18),
         mid = factor(married + 1, labels = c("single", "married")))

b6.9 <- brm(happiness ~ 0 + mid + a, 
            data = dat, family = gaussian,
            prior = c(prior(normal(0, 1), class = b, coef = midmarried),
                      prior(normal(0, 1), class = b, coef = midsingle),
                      prior(normal(0, 2), class = b, coef = a),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
            file = here::here("fits", "chp7", "b7h4-6.9"))

b6.10 <- brm(happiness ~ 1 + a, 
             data = dat, family = gaussian,
             prior = c(prior(normal(0, 1), class = Intercept),
                       prior(normal(0, 2), class = b, coef = a),
                       prior(exponential(1), class = sigma)),
             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
             file = here::here("fits", "chp7", "b7h4-6.10"))

```

```{r}
b6.9 <- add_criterion(b6.9, criterion = "loo")
b6.10 <- add_criterion(b6.10, criterion = "loo")

loo_compare(b6.9, b6.10)
#>       elpd_diff se_diff
#> b6.9     0.0       0.0 
#> b6.10 -194.0      17.6
```

PSIS shows a strong preference for `b6.9`, which is the model that includes both age and marriage status. However, `b6.10` provides the correct causal inference, as no additional conditioning is needed.

```{r}
adjustmentSets(hma_dag, exposure = "A", outcome = "H")
#> {}
```

The reason is that in this model, martial status is collider. Adding this variable to the model add a real statistical association between happiness and age, which improves the predictions that are made. However, the association is not causal, so interventing on age (if that were possible), would not actually change happiness. Therefore it's important to consider the causal implications of your model before selecting one based on PSIS or WAIC alone.

## **5**

:::question
>Revisit the urban fox data, `data(foxes)`, from the previous chapter's practice problems. Use WAIC or PSIS based model comparison on five different models, each using `weight` as the outcome, and containing these sets of predictor variables:
1. `avgfood + groupsize + area`
2. `avgfood + groupsize`
3. `groupsize + area`
4. `avgfood`
5. `area`
Can you explain the relative differences in WAIC scores, using the fox DAG from the previous chapter? Be sure to pay attention to the standard error of the score differences `(dSE)`
:::

**Answer:**

First, let's estimate the five models.

```{r}
WAvGA <- dagitty("dag{
                 W <- Av <- G <- A
                 Av <- A}")
coordinates(WAvGA) <- list(x = c(W = 2, Av = 1, G = 2, A = 3),
                           y = c(W = 3, Av = 2, G = 1, A = 2))

drawdag(WAvGA)
```

1. `avgfood + groupsize + area`
2. `avgfood + groupsize`
3. `groupsize + area`
4. `avgfood`
5. `area`

```{r}
data(foxes)

fox_dat <- foxes %>% 
  as_tibble() %>% 
  select(-group) %>% 
  mutate(across(everything(), standardize))

# avgfood + groupsize + area
b7h5_1 <- brm(weight ~ 1 + avgfood + groupsize + area,
              data = fox_dat, family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
              file = here::here("fits", "chp7", "b7h5_1"))

# avgfood + groupsize 
b7h5_2 <- brm(weight ~ 1 + avgfood + groupsize,
              data = fox_dat, family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
              file = here::here("fits", "chp7", "b7h5_2"))

# groupsize + area
b7h5_3 <- brm(weight ~ 1 + groupsize + area,
              data = fox_dat, family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
              file = here::here("fits", "chp7", "b7h5_3"))

# avgfood
b7h5_4 <- brm(weight ~ 1 + avgfood,
              data = fox_dat, family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
              file = here::here("fits", "chp7", "b7h5_4"))

#  area
b7h5_5 <- brm(weight ~ 1 + area,
              data = fox_dat, family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
              file = here::here("fits", "chp7", "b7h5_5"))
```

```{r}
b7h5_1 <- add_criterion(b7h5_1, criterion = "waic")
b7h5_2 <- add_criterion(b7h5_2, criterion = "waic")
b7h5_3 <- add_criterion(b7h5_3, criterion = "waic")
b7h5_4 <- add_criterion(b7h5_4, criterion = "waic")
b7h5_5 <- add_criterion(b7h5_5, criterion = "waic")

comp <- loo_compare(b7h5_1, b7h5_2, b7h5_3, b7h5_4, b7h5_5, criterion = "waic")
comp
#>        elpd_diff se_diff
# b7h5_1  0.0       0.0   
# b7h5_2 -0.5       1.7   
# b7h5_3 -0.5       1.4   
# b7h5_4 -5.4       3.4   
# b7h5_5 -5.6       3.4  
```

In general, the models don’t appear all that different from each other. However, there does seem to be two groups of models: `b7h5_1`, `b7h5_2`, and `b7h5_3` are all nearly identical; and `b7h5_4` and `b7h5_5` are nearly identical.

```{r}
plot_comp <- comp %>%
  as_tibble(rownames = "model") %>%
  mutate(across(-model, as.numeric),
         model = fct_inorder(model))

waic_val <- plot_comp %>%
  select(model, waic, se = se_waic) %>%
  mutate(lb = waic - se,
         ub = waic + se)

diff_val <- plot_comp %>%
  select(model, waic, se = se_diff) %>%
  mutate(se = se * 2) %>%
  mutate(lb = waic - se,
         ub = waic + se) %>%
  filter(se != 0)

ggplot() +
  geom_pointrange(data = waic_val, mapping = aes(x = waic, xmin = lb, xmax = ub,
                                                 y = fct_rev(model))) +
  geom_pointrange(data = diff_val, mapping = aes(x = waic, xmin = lb, xmax = ub,
                                                 y = fct_rev(model)),
                  position = position_nudge(y = 0.2), shape = 2,
                  color = "#009FB7") +
  labs(x = "Deviance", y = NULL)
```

```{r}
WAvGA <- dagitty("dag{
                 W <- Av -> G -> W
                 Av <- A}")
coordinates(WAvGA) <- list(x = c(W = 2, Av = 1, G = 3, A = 2),
                           y = c(W = 3, Av = 2, G = 2, A = 1))

drawdag(WAvGA)
```


The first three models (`b7h5_1`, `b7h5_2`, and `b7h5_3`) all contain `groupsize` and one or both of `area` and `avgfood`. The reason these models is the same is that there are no back-door path from `area` or `avgfood` to `weight`. In other words, the effect of `area` adjusting for `groupsize` is the same as the effect of `avgfood` adjusting for `groupsize`, because the effect of `area` is routed entirely through `avgfood`.

Similarly, the last two models (`b7h5_4` and `b7h5_5`) are also nearly identical because of the relationship of `area` to `avgfood`. Because the effect of `area` is routed entirely through `avgfood`, including only `avgfood` or `area` should result in the same inferences.






