---
title: "Untitled"
author: "Hoang-Chau Luong"
date: '2022-09-09'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Medium**

## **1**

Modify the DAG on page 186 to include the variable `V`, an unobserved cause of `C` and `Y:C ← V → Y`. Reanalyze the DAG. How many paths connect X to Y? Which must be closed? Which variables should you condition on now?

```{r}
library(rethinking)
library(dagitty)
library(ggdag)
library(tidyverse)
library(tidybayes)
library(brms)

dag_6M1 <- dagitty("dag{
                   U [unobserved]
                   V [unobserved]
                   X -> Y
                   X <- U -> B <- C -> Y
                   U <- A -> C
                   C <- V -> Y
                   }")

coordinates(dag_6M1) <- list(
  x = c(X = 0, Y = 2, U = 0, A = 1, B = 1, C = 2, V = 2.5),
  y = c(X = 2, Y = 2, U = 1, A = 0.5, B = 1.5, C = 1, V = 1.5)
)

drawdag(dag_6M1)
```

1)  List the path:
2)  Backdoor
3)  Adjustment set

-   X -\> X (The causal path)

-   X \<- U -\> B \<- C -\> Y (backdoor) =\> This path is closed because B is a collider

-   X \<- U -\> B \<- C \<- V -\> Y (backdoor) =\> This path is closed because B is a collider

-   X \<- U \<- A -\> C -\> Y (backdoor) =\> Condition on A to close this path

-   X \<- U \<- A -\> C \<- V -\> Y (backdoor) =\> This path is closed because C is a collider

    We want to leave path 1 open and make sure all of the others are closed, because they are non-causal paths that will confound inference. As before, all the paths through B are already closed, since B is a collider. So we don't condition on B. Similarly, the new paths through both A and V are closed, because C is a collider on those paths. So it's enough to condition on A to close all non-causal paths

    If you like, you can check using dagitty:

    ```{r}
    adjustmentSets( dag_6M1 , exposure="X" , outcome="Y" )
    # { A }
    ```

Note that in the chapter, where V is absent, it is also fine to condition on C. It isn't now.

## **2**

::: question
> Sometimes, in order to avoid multicollinearity, people inspect pairwise correlations among predictors before including them in a model. This is a bad procedure, because what matters is the conditional association, not the association before the variables are included in the model. To highlight this, consider the DAG $X \rightarrow Z \rightarrow Y$. Simulate data from this DAG so that the correlation between $X$ and $Z$ is very large. Then include both in a model prediction $Y$. Do you observe any multicollinearity? Why or why not? What is different from the legs example in the chapter?
:::

```{r}
n <- 1e4
set.seed(11)
X <- rnorm(n)
Z <- rnorm(n, 2*X, 0.2)
cor(X, Z)

Y <- rnorm(n, Z)
cor(Y, Z)

df <- tibble(X = standardize(X),
             Y = standardize(Y), 
             Z = standardize(Z))

lm_YX  <- lm(Y ~ X, data = df)
lm_YZ  <- lm(Y ~ Z, data = df)
lm_YXZ <- lm(Y ~ X + Z, data = df)

summary(lm_YX)
summary(lm_YZ)
summary(lm_YXZ)

m_6M2 <- quap(
  alist(
    Y ~ dnorm( mu , sigma ),
    mu <- a + bX*X + bZ*Z,
    c(a,bX,bZ) ~ dnorm(0,1),
    sigma ~ dexp(1)
  ) , data=list(X=X,Y=Y,Z=Z) )
precis( m_6M2 )
```

Simulate with large sample

```{r}
f <- function(n = 1e4, bXY = 1, bXZ = 2, bZY = 1){
  X <- rnorm(n)
  Z <- rnorm(n, bXZ*X)
  Y <- rnorm(n, bZY*Z)
  bX <- coef(lm(Y ~ X))['X']
  bXZ <- coef(lm(Y ~ X + Z))['X']
  return(c(bX, bXZ))
}
```

```{r}
set.seed(1981)

n <- 1e4
df <- tibble(x = rnorm(n)) %>% 
  mutate(z = rnorm(n, mean = x, sd = 0.1),
         y = rnorm(n, mean = z),
         across(everything(), standardize))

sim_cor <- cor(df$x, df$z)

b6m2 <- brm(y ~ 1 + x + z, data = df, family = gaussian,
            prior = c(prior(normal(0, 0.2), class = Intercept),
                      prior(normal(0, 0.5), class = b),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here::here("fits", "chp6", "b6m2"))

as_draws_df(b6m2) %>%
  as_tibble() %>% 
  select(b_Intercept, b_x, b_z, sigma) %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value, y = name)) +
  stat_halfeye(.width = c(0.67, 0.89, 0.97))
```

# **Hard**

## **1**

::: question
> Use the Waffle House data, `data(WaffleDivorce), to find the total causal inference of number of Waffle Houses on divorce rate. Justify your model or models with a causal graph
:::

For this problem, we'll use the DAG given in the chapter example:

```{r}
waffle_dag <- dagitty("dag{
                      S -> W -> D <- A -> M -> D
                      S -> A
                      S -> M
}")

coordinates(waffle_dag) <- list(x = c(A = 1, S = 1, M = 2, W = 3, D = 3),
                                y = c(A = 1, S = 3, M = 2, W = 3, D = 1))

ggplot(waffle_dag, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_text(color = "black", size = 10) +
  geom_dag_edges(edge_color = "black", edge_width = 2,
                 arrow_directed = grid::arrow(length = grid::unit(15, "pt"),
                                              type = "closed")) +
  theme_void()
```

   In order to estimate total causal effect of Waffle House (W) on divorce rate (D), we have to condition on either S or both A and M. For simplicity, we'll condition on only S.
   
```{r}
impliedConditionalIndependencies(waffle_dag)

adjustmentSets(waffle_dag, exposure = "W", outcome = "D")
```
   
```{r}
data("WaffleDivorce")
waffle <- WaffleDivorce %>%
  as_tibble() %>%
  select(D = Divorce,
         A = MedianAgeMarriage,
         M = Marriage,
         S = South,
         W = WaffleHouses) %>%
  mutate(across(-S, standardize),
         S = factor(S))

waff_mod <- brm(D ~ 1 + W + S, data = waffle, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                file = here::here("fits", "chp6", "b6h1"))
```

  Finally, we can see the causal estimate of Waffle Houses on divorce rate by looking at the posterior distribution of the `b_W` parameter.
  
  Here, we can see that the estimate is very small, indicating that the number of Waffle Houses has little to no causal impact on the divorce rate in a state.

```{r}
spread_draws(waff_mod, b_W) %>% 
  ggplot(aes(x = b_W)) +
  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +
  labs(x = expression(beta[W]), y = "Density")
```

## **2**

::: question
> Build a series of models to test the implied conditional independencies of the causal graph you used in the previous problem. If any of the tests fail, how do you think the graph needs to be ammended? Does the graph need more or fewer arrows? Feel free to nominate variables that aren’t in the data.
:::
  
```{r}
waff_ci1 <- brm(A ~ 1 + W + S, 
                data = waffle, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
                file = here::here("fits", "chp6", "b6h2-1"))

waff_ci2 <- brm(D ~ 1 + S + A + M + W,
                data = waffle, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
                file = here::here("fits", "chp6", "b6h2-2"))

waff_ci3 <- brm(M ~ 1 + W + S,
                data = waffle, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
                file = here::here("fits", "chp6", "b6h2-3"))
```

Finally, we can estimate the posterior distributions for the implied conditional independence. The implied conditional independency tested in the first model A⊥⊥W|S, appears to be met, as the $\beta_{W}$ coefficient from the model is centered on zero. The same is true for the third implied conditional independency, M⊥⊥W|S. The second implied conditional independency, $D⊥⊥S|A,M,W$, is less clear, as the posterior distribution overlaps zero, but does indicate a *slightly positive* relationship between divorce rate a state’s “southern” status, even after conditioning on median age of marriage, marriage rate, and the number of waffle houses. This is likely because there are other variables missing from the model that are related to both divorce rate and also southerness. For example, `religiosity`, `family size`, and `education` could all plausibly impact divorce rates and show regional differences in the United States.
 
```{r}
lbls <- c(expression("Model 1:"~beta[W]),
          expression("Model 2:"~beta[S]),
          expression("Model 3:"~beta[W]))

bind_rows(
  gather_draws(waff_ci1, b_W) %>%
    ungroup() %>%
    mutate(model = "ICI 1"),
  gather_draws(waff_ci2, b_S1) %>%
    ungroup() %>%
    mutate(model = "ICI 2"),
  gather_draws(waff_ci3, b_W) %>%
    ungroup() %>%
    mutate(model = "ICI 3")
) %>%
  ggplot(aes(x = .value, y= model)) +
  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +
  scale_y_discrete(labels = lbls) +
  labs(x = "Parameter Estimate", y = "Implied Conditional Independency")
```


## **3**

>All three problems below are based on the same data. The data in `data(foxes)` are 116 foxes from 30 different urban groups in England. These foxes are like street gangs. Group size varies from 2 to 8 individuals. Each group maintains its own urban territory. Some territories are larger than others. The `area` variable encodes this information. Some territories also have more `avgfood` than others. We want to model the `weight` of each fox. For the problems below, assume the following DAG:

::: question
> Use a model to infer the total causal influence of area on weight. Would increasing the area available to each fox make it heavier (healthier)? You might want to standardize the variables. Regardless, use prior predictive simulation to show that your model’s prior predictions stay within the possible outcome range
:::

First let's load the data and standardize the variables

```{r}
data("foxes")

fox_dat <- foxes %>% 
  as_tibble() %>% 
  select(area, avgfood, groupsize, area, weight) %>% 
  mutate(across(everything(), standardize))

fox_dat
```

```{r}
fox_dag <- dagitty("dag{
                   area -> avgfood -> groupsize -> weight
                   avgfood -> weight
}")

coordinates(fox_dag) <- list(x = c(avgfood = 1, weight = 2, area = 2, groupsize = 3),
                             y = c(avgfood = 2, weight = 1, area = 3, groupsize = 2))

adjustmentSets(fox_dag, exposure = "area", outcome = "weight")
```

```{r}
set.seed(2020)

n <- 1000
tibble(group = seq_len(n),
       alpha = rnorm(n, 0, 0.2),
       beta  = rnorm(n, 0, 0.5)) %>% 
  expand(nesting(group, alpha, beta),
         area = seq(from = -2, to = 2, length.out = 100)) %>% 
  mutate(weight = alpha + beta * area) %>% 
  ggplot(aes(x = area, y = weight, group = group)) +
  geom_line(alpha = 1/10) +
  geom_hline(yintercept = c((0 - mean(foxes$weight)) / sd(foxes$weight),
                            (max(foxes$weight) - mean(foxes$weight))/ sd(foxes$weight)), linetype = c("dashed", "solid"), color = "red") +
  annotate(geom = "text", x = -2, y = -3.83, hjust = 0, vjust = 1,
           label = "No weight") +
  annotate(geom = "text", x = -2, y = 2.55, hjust = 0, vjust = 0,
           label = "Maximum weight") +
  expand_limits(y = c(-4, 4)) +
  labs(x = "Standardized Area", y = "Standardized Weight")
```

Finally, we can estimate the model. Conditional on this DAG and our data, it doesn’t appear as though area size has any causal effect on the weight of foxes in this sample.

```{r}
area_mod <- brm(weight ~ 1 + area, 
                data = fox_dat, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
                file = here::here("fits", "chp6", "b6h3"))

as_draws_df(area_mod) %>% 
  as_tibble() %>% 
  select(b_Intercept, b_area, sigma) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("b_Intercept", "b_area", "sigma"))) %>% 
  ggplot(aes(x = value, y = fct_rev(name))) +
  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +
  labs(x = "Parameter Estimate", y = "Parameter")

data(foxes)
d <- foxes
d$W <- standardize(d$weight)
d$A <- standardize(d$area)
m1 <- quap(
  alist(
    W ~ dnorm( mu , sigma ),
    mu <- a + bA*A,
    a ~ dnorm(0,0.2),
    bA ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data=d )
precis(m1)

#       mean   sd  5.5% 94.5%
# a     0.00 0.08 -0.13  0.13
# bA    0.02 0.09 -0.13  0.16
# sigma 0.99 0.06  0.89  1.09
```

Territory size seems to have no total causal influence on weight, at least not in this sample.

## **4**

::: question
> Now infer the causal impact of adding food to a territory. Would this make foxes heavier? Which covariates do you need to adjust for to estimate the total causal influence of food?
:::

To estimate the total causal impact of food, we don’t need to adjust for any variables. There is a direct path from average food to weight and an indirect path through group size. If we condition on group size, then that path would be closed and we would be left with only the direct effect. However, as we want the total effect, there is no adjustment to be made.

```{r}
adjustmentSets(fox_dag, exposure = "avgfood", outcome = "weight")
```

When we estimate the model regressing weight on average food available, we see that there is no effect of food on weight. Given the DAG, this is expected. If there were an impact of food on weight, then we would have expected to see an impact of area on weight in the previous problem, as area is upstream of average food in the causal model.

```{r}
food_mod <- brm(weight ~ 1 + avgfood, 
                data = fox_dat, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 123,
                file = here::here("fits", "chp6", "b6h4"))

as_draws_df(food_mod) %>% 
  as_tibble() %>% 
  select(b_Intercept, b_avgfood, sigma) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("b_Intercept", "b_avgfood", "sigma"))) %>%
  ggplot(aes(x = value, y = fct_rev(name))) +
  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +
  labs(x = "Parameter Value", "Parameter")
```


```{r}
d$W <- standardize(d$weight)
d$aF <- standardize(d$avgfood)
m2 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bF*aF,
    a ~ dnorm(0, 0.2),
    bF ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

precis(m2)
```

Again nothing. Adding food does not change weight. This shouldn’t surprise you, if the DAG is correct, because `area` is upstream of `avgfood`.

## **5**

::: question
> Now infer the causal impact of group size. Which covariates do you need to adjust for? Looking at the posterior distribution of the resulting model, what do you think explains these data? That is, can you explain the estimates for all three problems? How do they go together?
:::

When assessing the causal impact of group size, there is one back-door path: `G ← F → W`. In this path, average food, `F`, is a fork, so we have to condition on it to isolate the effect of group size.

```{r}
adjustmentSets(fox_dag, exposure = "groupsize", outcome = "weight")
#> { avgfood }
```

When we estimate this model, we see a negative effect for group size when controlling for food. We also now see a positive effect for average food when controlling for group size. Thus, the causal effect of group size is to decrease weight. Logically this makes sense, as there would be less food for each fox. This model also indicates that the direct effect of average food is to increase weight. That is, if group size is held constant, more food results in more weight. However, the total causal effect of food on weight, as we saw in the last problem, is nothing. This is because more food also leads to larger groups, which in turn decreases weight. This is a masking effect, also called a `post-treatment bias`.

```{r}
grp_mod <- brm(weight ~ 1 + avgfood + groupsize, data = fox_dat,
               family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b,),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
               file = here::here("fits", "chp6", "b6h5"))

as_draws_df(grp_mod) %>%
  as_tibble() %>%
  select(b_Intercept, b_avgfood, b_groupsize, sigma) %>%
  pivot_longer(everything()) %>%
  mutate(name = factor(name, levels = c("b_Intercept", "b_avgfood",
                                        "b_groupsize", "sigma"))) %>%
  ggplot(aes(x = value, y = fct_rev(name))) +
  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +
  labs(x = "Parameter Estimate", y = "Parameter")
```


The variable `groupsize` does have a back-door path, passing through `avgfood`. So to infer the causal influence of `groupsize`, we need to close that path. This implies a model with both `groupsize` and `avgfood` as predictors

```{r}
m3 <- quap(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + bG*groupsize + bF*avgfood,
    a ~ dnorm(0, 0.2),
    bG ~ dnorm(0, 0.5),
    bF ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = fox_dat
)

precis(m3)
```


It looks like group size is negatively associated with weight, controlling for food. Similarly, food is positively associated with weight, controlling for group size. So the causal influence of group size is to reduce weight—less food for each fox. And the direct causal influence of food is positive, of course. But the total causal influence of food is still nothing, since it causes larger groups. This is a masking effect, like in the milk energy example. But the causal explanation here is that more foxes move into a territory until the food available to each is no better than the food in a neighboring territory. Every territory ends up equally good/bad on average. This is known in behavioral ecology as an `ideal free distribution`.


# **More Homework**

:::question
> **1.** The first two problems are based on the same data. The data in `data(foxes)` are 116 foxes from 30 different urban groups in England. These fox groups are like street gangs. Group size (`groupsize`) varies from 2 to 8 individuals. Each group maintains its own (almost exclusive) urban territory. Some territories are larger tahn others. The `area` variable encodes this information. Some territories also have more `avgfood` than others. And food influences the `weight` of each fox. Assume this DAG:
> where $F$ is `avgfood`, $G$ is `groupsize`, $A$ is `area`, and $W$ is `weight`.
> Use the backdoor criterion and estimate the total causal influence of $A$ on $F$. What effect would increasing the area of a territory have on the amount of food inside it?
:::

There is only one path from A to F, the direct path. Thus there are no backdoors, and no addtional variables that need to be included in the model. We can confirm with dagitty.

```{r}
adjustmentSets(fox_dag, exposure = "area", outcome = "avgfood")
```

```{r}
data(foxes)

fox_dat <- foxes %>%
  as_tibble() %>%
  select(area, avgfood, weight, groupsize) %>%
  mutate(across(everything(), standardize))

food_on_area <- brm(avgfood ~ 1 + area, data = fox_dat, family = gaussian,
                    prior = c(prior(normal(0, 0.2), class = Intercept),
                              prior(normal(0, 0.5), class = b,),
                              prior(exponential(1), class = sigma)),
                    iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                    file = here::here("fits", "chp6", "w3h1"))

summary(food_on_area)
#>  Family: gaussian 
#>   Links: mu = identity; sigma = identity 
#> Formula: avgfood ~ 1 + area 
#>    Data: fox_dat (Number of observations: 116) 
#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;
#>          total post-warmup draws = 8000
#> 
#> Population-Level Effects: 
#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> Intercept     0.00      0.04    -0.09     0.09 1.00     8076     6131
#> area          0.88      0.04     0.79     0.96 1.00     8389     5927
#> 
#> Family Specific Parameters: 
#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> sigma     0.48      0.03     0.42     0.54 1.00     7997     6226
#> 
#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
#> and Tail_ESS are effective sample size measures, and Rhat is the potential
#> scale reduction factor on split chains (at convergence, Rhat = 1).
```

We see a fairly strong effect of area on the average amount of food. The credible interval for area is well above zero. For an increase of 1 standard deviation in area we would expect to see about a .9 standard deviation increase in food. Logically this makes sense, as a greater area means there are would be more prey available.

:::question
> **2.** Now infer both the **total** and **direct** causal effects of adding food $F$ to a territory on the weight $W$ of foxes. Which covariates do you need to adjust for in each case? In light of your estimates from this problem and the previous one, what do you think is going on with these foxes? Feel free to speculate---all that matters is that you justify your speculation.
:::

There are no backdoor paths from $F$ to $W$, so there are no covariates that need to be added when evaluating the total effect of food on weight. If we want the direct effect, then we need to close the path of $F \rightarrow G \rightarrow W$ by adding group size as a covariate. We can again confirm with dagitty.

```{r}
adjustmentSets(fox_dag, exposure = "avgfood", outcome = "weight", effect = "total")
#>  {}

adjustmentSets(fox_dag, exposure = "avgfood", outcome = "weight", effect = "direct")
#> { G }
```

Now we estimate our models. First the total effect. In this model we see basically no effect of food on weight. The 95% interval in the {brms} output is -0.21 to 0.15 with a mean of -0.02. So the effect is just as likely to be positive as negative.

```{r}
food_total <- brm(weight ~ 1 + avgfood, data = fox_dat, family = gaussian,
                  prior = c(prior(normal(0, 0.2), class = Intercept),
                            prior(normal(0, 0.5), class = b,),
                            prior(exponential(1), class = sigma)),
                  iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                  file = here::here("fits", "chp6", "w3h2-total"))

summary(food_total)
#>  Family: gaussian 
#>   Links: mu = identity; sigma = identity 
#> Formula: weight ~ 1 + avgfood 
#>    Data: fox_dat (Number of observations: 116) 
#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;
#>          total post-warmup draws = 8000
#> 
#> Population-Level Effects: 
#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> Intercept    -0.00      0.08    -0.17     0.16 1.00     7369     5854
#> avgfood      -0.02      0.09    -0.21     0.15 1.00     7826     6205
#> 
#> Family Specific Parameters: 
#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> sigma     1.01      0.07     0.89     1.15 1.00     7839     6107
#> 
#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
#> and Tail_ESS are effective sample size measures, and Rhat is the potential
#> scale reduction factor on split chains (at convergence, Rhat = 1).
```

Next we estimate the direct effect. Here we see that when we stratify by group size, we see a strong positive effect of food on weight. This indicates that within a given group size, more food is associated with more weight.

```{r}
food_direct <- brm(weight ~ 1 + avgfood + groupsize, data = fox_dat,
                   family = gaussian,
                   prior = c(prior(normal(0, 0.2), class = Intercept),
                             prior(normal(0, 0.5), class = b,),
                             prior(exponential(1), class = sigma)),
                   iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                   file = here::here("fits", "chp6", "w3h2-direct"))

summary(food_direct)
#>  Family: gaussian 
#>   Links: mu = identity; sigma = identity 
#> Formula: weight ~ 1 + avgfood + groupsize 
#>    Data: fox_dat (Number of observations: 116) 
#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;
#>          total post-warmup draws = 8000
#> 
#> Population-Level Effects: 
#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> Intercept    -0.00      0.08    -0.16     0.16 1.00     5680     4403
#> avgfood       0.47      0.18     0.11     0.84 1.00     4022     3986
#> groupsize    -0.57      0.19    -0.93    -0.21 1.00     4021     4085
#> 
#> Family Specific Parameters: 
#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#> sigma     0.96      0.06     0.85     1.09 1.00     5544     5204
#> 
#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
#> and Tail_ESS are effective sample size measures, and Rhat is the potential
#> scale reduction factor on split chains (at convergence, Rhat = 1).
```

Altogether, these results seem to suggest a masking effect. That is, as more food is available, more foxes move into the territory, increasing the group size. This continues until an equilibrium is reached where the amount of food available is equally good (or equally bad) within each territory. Thus, the total effect of food is negligible because as more food becomes available, the group size increases such that the amount of food available for each individual fox remains relatively stable.

:::question
> **3.** Reconsider the Table 2 Fallacy example (from Lecture 6), this time with an unobserved confound $U$ that influences both smoking $S$ and stroke $Y$. Here's the modified DAG:
> First use the backdoor criterion to determine and adjustment set that allows you to estimate the causal effect of $X$ on $Y$, i.e., $P(Y|\text{do}(X))$. Second explain the proper interpretation of each coefficient implied by the regression model that corresponds to the adjustment set. Which coefficients (slopes) are causal and which are not? There is no need to fit any models. Just think through the implications.
:::

There are 5 paths from X to Y:

*X -> Y
*X <- S -> Y
*X <- A -> Y
*X <- A -> S -> Y
*X <- A -> S <- U -> Y

Path 1 is the effect of X on Y, which we want. The rest are backdoor paths which must be closed. Conditioning on A and S will close all of the paths, which we can confirm with dagitty.

```{r}
stroke_dag <- dagitty("dag{
  A -> S -> X -> Y;
  A -> X; A -> Y; S -> Y;
  S <- U -> Y 
  X [exposure]
  Y [outcome]
  U [unobserved]
}")

adjustmentSets(stroke_dag)
#> { A, S }
```

However, the unobserved variable $U$ has important implications for the interpretation of the model. There are three slope coefficients that would be in the canonical Table 2: $\beta_X$, $\beta_S$, and $\beta_A$. The focus of our investigation, $\beta_X$ represents the causal effect of $X$ on $Y$. However, because $S$ is a collider, both $\beta_S$ and $\beta_A$ are confounded by $U$. Thus, these coefficients should not be interpreted as any type of causal effect.

:::question
> **4 - OPTIONAL CHALLENGE.** Write a synthetic data simulation for the causal model shown in **Problem 3**. Be sure to include the unobserved confound in the simulation. Choose any functional relationships that you like---you don't have to get the epidemiology correct. You just need to honor the causal structure. Then design a regression model to estimate the influence of $X$ on $Y$ and use it on your synthetic data. How large of a sample do you need to reliably estimate $P(Y|\text{do}(X))$? Define "reliably" as you like, but justify your definition.
:::

Since, we're going to simulate a bunch of data sets, we'll create a function that will generate a single data based on the DAG. 

* The function allows us to specify a sample size and the relationship between X and Y. Notice that although U is used to generate the data, the variable is removed before return the data set, as it is unobserved 

```{r}
# S smoking
# U unobserved 
# A age
# X hiv
# Y stroke

# bx is direct effect coefficient
sim_dat <- function(n = 100, bx = 0){
  tibble(U = rnorm(n),
         A = rnorm(n)) %>% 
    mutate(S = rnorm(n, A + U, 1),
           X = rnorm(n, A + S, 1),
           Y = rnorm(n, A + S + (bx * X) + U, 1)) %>% 
    select(-U) %>% 
    mutate(across(everything(), standardize))
}
```

We’ll test one example data set. We use the sim_dat() function we just created to generate a data set with 10 subjects and a positive relationship between X and Y of 0.3

```{r}
set.seed(208)

dat1 <- sim_dat(n = 10, bx = 0.3)

mod1 <- brm(Y ~ 1 + X + S + A, data = dat1, family = gaussian,
            prior = c(prior(normal(0, 0.2), class = Intercept),
                      prior(normal(0, 0.5), class = b,),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here::here("fits", "chp6", "w3h4"))

summary(mod1, prob = 0.89)
#>  Family: gaussian 
#>   Links: mu = identity; sigma = identity 
#> Formula: Y ~ 1 + X + S + A 
#>    Data: dat1 (Number of observations: 10) 
#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;
#>          total post-warmup draws = 8000
#> 
#> Population-Level Effects: 
#>           Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS
#> Intercept     0.00      0.09    -0.14     0.14 1.00     5459     4269
#> X             0.31      0.28    -0.13     0.75 1.00     4000     3847
#> S             0.61      0.22     0.24     0.96 1.00     4549     4029
#> A             0.05      0.22    -0.29     0.42 1.00     4558     4417
#> 
#> Family Specific Parameters: 
#>       Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS
#> sigma     0.31      0.10     0.19     0.48 1.00     3566     4676
#> 
#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
#> and Tail_ESS are effective sample size measures, and Rhat is the potential
#> scale reduction factor on split chains (at convergence, Rhat = 1).

as_draws_df(mod1) %>% 
  as_tibble() %>% 
  select(b_Intercept, b_X, b_S, b_A, sigma) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("b_Intercept", "b_X", "b_S", "b_A", "sigma"))) %>% 
  ggplot(aes(x = value, y = fct_rev(name))) +
  stat_halfeye()
```

And it worked! We got back an estimate of X very close to 0.3, and equally likely to be positive or negative. This was with a sample size of only 10.

```{r}
sim_func <- function(n, bx = 0) {
  dat <- sim_dat(n = n, bx = bx)
  
  suppressMessages(output <- capture.output(
    mod <- brm(Y ~ 1 + X + S + A, data = dat, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b,),
                         prior(exponential(1), class = sigma)),
               refresh = 0,
               iter = 4000, warmup = 2000, chains = 4, cores = 4)
  ))
  
  as_draws_df(mod, "b_X") %>% 
    mean_hdi(.width = 0.89) %>% 
    mutate(abs_error = abs(b_X - bx),
           int_width = .upper - .lower) %>% 
    select(abs_error, int_width)
}

sim_results <- expand_grid(sample_size = c(10, 20, 50, 100, 250, 500, 1000),
                           replication = seq_len(100)) %>% 
  mutate(results = map(sample_size, sim_func, bx = 0.3)) %>% 
  unnest(results) %>% 
  write_rds(here::here("fits", "chp6", "w3h4-sim.rds"))

sim_results %>% 
  select(-replication) %>% 
  pivot_longer(-sample_size, names_to = "measure", values_to = "value") %>% 
  mutate(measure = factor(measure, levels = c("abs_error", "int_width"),
                          labels = c("Absolute Error", "89% Interval Width"))) %>% 
  ggplot(aes(x = factor(sample_size), y = value)) +
  facet_wrap(~measure, nrow = 1) +
  stat_interval(.width = c(0.67, 0.89, 0.97)) +
  scale_color_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                     breaks = c(0.67, 0.89, 0.97)) +
  labs(x = "Sample Size", y = "Value", color = "Interval")
```

































